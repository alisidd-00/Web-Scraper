venv\Scripts\activate

pip freeze > requirements.txt


# Define your list of proxies
proxies = [
    'http://114.156.77.107:8080',
    'http://20.111.54.16:8123',
    'http://113.160.132.195:8080',
    'http://20.111.54.16:8123',
    'http://113.160.132.195:8080',
    'http://103.45.64.88:8080',
    'http://222.252.194.29:8080',
    'http://68.107.241.150:8080',
    'http://109.68.148.54:3128',
    'http://157.254.53.50:80',
    'http://46.47.197.210:3128',
    'http://43.134.121.40:3128',
    'http://160.86.242.23:8080',
    'http://65.108.159.129:1080',
    'http://43.153.207.93:3128',
    'http://158.255.77.169:80',
    'http://43.200.77.128:3128',
    'http://83.68.136.236:80',
    'http://179.96.28.58:80',
    'http://198.44.255.5:80',
    'http://47.88.59.79:82',
    'http://51.254.78.223:80',
    'http://43.203.253.129:8004',
    'http://18.231.176.166:8363',
    # Add more proxies as needed
]



import time
import random
import pandas as pd
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium import webdriver
from webdriver_manager.chrome import ChromeDriverManager




# Function to scrape data from a company page
def scrape_company_data(driver, company_url):
    try:
        driver.get(company_url)
        time.sleep(random.uniform(2, 4))  # wait for the page to load
        
        name = driver.find_element(By.CSS_SELECTOR, 'h2 span.titleSpan').text if driver.find_elements(By.CSS_SELECTOR, 'h2 span.titleSpan') else 'Not Available'
        phone = driver.find_element(By.CSS_SELECTOR, 'a.phone').text if driver.find_elements(By.CSS_SELECTOR, 'a.phone') else 'Not Available'
        email = driver.find_element(By.CSS_SELECTOR, 'a.email').text if driver.find_elements(By.CSS_SELECTOR, 'a.email') else 'Not Available'

        return {
            'Name': name,
            'Phone': phone,
            'Email': email
        }
    except Exception as e:
        print(f"Error while scraping {company_url}: {e}")
        return {
            'Name': 'Not Available',
            'Phone': 'Not Available',
            'Email': 'Not Available'
        }

# Main function to scrape the main page
def main():
    # Setup Brave options
    options = Options()
    chrome_options = webdriver.ChromeOptions()
    options.binary_location = 'C:/Program Files/BraveSoftware/Brave-Browser/Application/brave.exe'  # Adjust this path if necessary

# Get the current version of Brave and use it
    #driver = webdriver.Chrome(service=Service(ChromeDriverManager(version="130.0.6723.58").install()), options=options)
    #driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)
    #driver = webdriver.Chrome(ChromeDriverManager().install()) 
    driver = webdriver.Chrome(options=chrome_options)
    
    
    
    base_url = "https://es.kompass.com/businessplace/z/de/"
    driver.get(base_url)
    company_data_list = []

    # Setup Selenium WebDriver with Brave
    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)

    try:
        driver.get(base_url)
        time.sleep(random.uniform(2, 4))  # wait for the page to load

        # Extract company links
        company_links = driver.find_elements(By.CSS_SELECTOR, 'a.company-link')

        for link in company_links:
            company_url = link.get_attribute('href')
            company_data = scrape_company_data(driver, company_url)
            company_data_list.append(company_data)

            time.sleep(random.uniform(1, 3))

        # Save data to CSV
        df = pd.DataFrame(company_data_list)
        df.to_csv('company_data.csv', index=False)

    except Exception as e:
        print(f"Error while accessing {base_url}: {e}")

    finally:
        driver.quit()  # Close the browser

if __name__ == "__main__":
    main()
